{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install numerapi pyarrow numerai-tools xgboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7acbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from numerai_tools.scoring import numerai_corr\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f53f9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Lets start by loading the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c064417",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION = \"v5.1\"\n",
    "napi = NumerAPI()\n",
    "\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "feature_set = feature_sets[\"all\"]\n",
    "\n",
    "# Download the training data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Download validation data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Download the validation benchmark models\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation_benchmark_models.parquet\")\n",
    "validation_benchmark_models = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation_benchmark_models.parquet\",\n",
    ")\n",
    "\n",
    "validation[\"benchmark_prediction\"] = validation[\"v51_lgbm_cyrusd20\"]\n",
    "validation = validation.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd2f3c",
   "metadata": {},
   "source": [
    "We keep the last 100th eras as test set for measuring performances.\n",
    "\n",
    "We fit the model on the rest of the eras.\n",
    "\n",
    "We remove the 4 first test eras to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the last 100th eras for test set\n",
    "test_era = validation[\"era\"].unique()[-100:]\n",
    "test = validation[validation[\"era\"].isin(test_era)]\n",
    "validation = validation[~validation[\"era\"].isin(test_era)]\n",
    "\n",
    "# Concatenate train and validation datasets for training\n",
    "train = pd.concat([train, validation.drop(\"benchmark_prediction\", axis=1)], ignore_index=True)\n",
    "\n",
    "\n",
    "# Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
    "# so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
    "last_train_era = int(train[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "\n",
    "test = test[~test[\"era\"].isin(eras_to_embargo)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d7779",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We observe the best model is the following one:\n",
    "\n",
    "TODO complete here\n",
    "\n",
    "Now we are gonna train on the full train and val eras and run the model on test eras to see its actual real performance.\n",
    "\n",
    "**NB** We should actually do rolling cross validation every week however this is costly and we don't expect it to give a major change so we are only gonna train the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c62c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=30000,\n",
    "    learning_rate=0.001,\n",
    "    max_depth=10,\n",
    "    colsample_bytree=0.1,\n",
    "    verbosity=0,\n",
    "    seed=42,\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "model.fit(train[feature_set], train[\"target\"])\n",
    "test[\"prediction\"] = model.predict(test[feature_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67df64a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "Numerai provides a benchmark model. We will use it to compare our performances on the usual metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1187f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(test, prediction_col):\n",
    "\n",
    "    # Compute per-era correlation\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        per_era_corr = test.groupby(\"era\").apply(\n",
    "            lambda x: numerai_corr(x[[prediction_col]].dropna(), x[\"target\"].dropna()),\n",
    "            include_groups=False\n",
    "        )\n",
    "        per_era_corr.fillna(0, inplace=True)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    corr_mean = per_era_corr.mean().item()\n",
    "    corr_std = per_era_corr.std(ddof=0).item()\n",
    "    corr_sharpe = corr_mean / corr_std if corr_std > 0 else np.nan\n",
    "    corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max().item()\n",
    "    corr_hit_rate = (per_era_corr > 0).mean().item()\n",
    "\n",
    "\n",
    "    # Display performance metrics\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Mean:           {corr_mean:>10.6f}\")\n",
    "    print(f\"Std:            {corr_std:>10.6f}\")\n",
    "    print(f\"Sharpe:         {corr_sharpe:>10.4f}\")\n",
    "    print(f\"Max Drawdown:   {corr_max_drawdown:>10.6f}\")\n",
    "    print(f\"Hit Rate:       {corr_hit_rate:>10.2%}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Compute feature exposures\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        feature_exposures = validation.groupby(\"era\").apply(\n",
    "            lambda d: d.drop(columns=[\"target\", \"prediction\"]).corrwith(d[\"prediction\"]),\n",
    "            include_groups=False\n",
    "        )\n",
    "        feature_exposures.fillna(0, inplace=True)\n",
    "\n",
    "    # Display feature exposure metrics\n",
    "    max_feature_exposure = feature_exposures.max(axis=1)\n",
    "    print(\"FEATURE EXPOSURE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Max Feature Exposure - Mean: {max_feature_exposure.mean():.4f}\")\n",
    "    print(f\"Max Feature Exposure - Std:  {max_feature_exposure.std():.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Plot the cumulative per-era correlation\n",
    "    per_era_corr.cumsum().plot(\n",
    "    title=\"Cumulative Validation CORR\",\n",
    "    kind=\"line\",\n",
    "    figsize=(8, 4),\n",
    "    legend=False\n",
    "    )\n",
    "\n",
    "    return per_era_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1353505",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_era_corr = evaluate_prediction(test, \"prediction\")\n",
    "\n",
    "per_era_corr.cumsum().plot(\n",
    "title=\"Cumulative Validation CORR on final model\",\n",
    "kind=\"line\",\n",
    "figsize=(8, 4),\n",
    "legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cf6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_era_corr = evaluate_prediction(test, \"benchmark_prediction\")\n",
    "\n",
    "per_era_corr.cumsum().plot(\n",
    "title=\"Cumulative Validation CORR on benchmark model\",\n",
    "kind=\"line\",\n",
    "figsize=(8, 4),\n",
    "legend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a7531",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "TODO complete the final analysis here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
