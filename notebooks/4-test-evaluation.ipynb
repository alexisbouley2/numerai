{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8fd232",
   "metadata": {},
   "source": [
    "- measure max feature exposure\n",
    "\n",
    "\n",
    "- measure cum corr\n",
    "\n",
    "- check ay metrics presented on the colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088a9af",
   "metadata": {},
   "source": [
    "Still unknown to do\n",
    "\n",
    "- ensemble method \n",
    "\n",
    "- fit on different targets ?\n",
    "\n",
    "- All the production automatization part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee408f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa09bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "The practical workflow:\n",
    "Phase 1 (Feature selection - THIS WEEK):\n",
    "\n",
    "Use fixed train/val split\n",
    "Test feature counts quickly\n",
    "Pick best configuration\n",
    "\n",
    "Phase 2 (Final validation - NEXT WEEK):\n",
    "\n",
    "Take your best feature set\n",
    "Run the rolling simulation above\n",
    "This gives you realistic live performance\n",
    "\n",
    "Why this works:\n",
    "\n",
    "Feature importance is relatively stable across time\n",
    "You're optimizing WHAT features to use, not WHEN to use them\n",
    "The rolling validation is just final confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a033f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install numerapi pyarrow numerai-tools lightgbm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f53f9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Lets start by loading the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c064417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "\n",
    "DATA_VERSION = \"v5.1\"\n",
    "napi = NumerAPI()\n",
    "\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "feature_set = feature_sets[\"all\"]\n",
    "\n",
    "# Download the training data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Download validation data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Concatenate train and validation datasets for training\n",
    "train_val = pd.concat([train, validation], ignore_index=True)\n",
    "\n",
    "#Keep the last 100th eras for test set\n",
    "test_era = train_val[\"era\"].unique()[-100:]\n",
    "\n",
    "test = train_val[train_val[\"era\"].isin(test_era)]\n",
    "train_val = train_val[~train_val[\"era\"].isin(test_era)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69258d33",
   "metadata": {},
   "source": [
    "We remove first test eras to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2256ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
    "# so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
    "last_train_era = int(train_val[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "\n",
    "test = test[~test[\"era\"].isin(eras_to_embargo)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d7779",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We observe the best model is the following one:\n",
    "\n",
    "TODO complete here\n",
    "\n",
    "Now we are gonna train on the full train and val eras and run the model on test eras to see its actual real performance.\n",
    "\n",
    "**NB** We should actually do rolling cross validation every week however this is costly and we don't expect it to give a major change so we are only gonna train the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c62c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from numerai_tools.scoring import numerai_corr\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "  n_estimators=2000,\n",
    "  learning_rate=0.01,\n",
    "  max_depth=5,\n",
    "  num_leaves=2**5-1,\n",
    "  colsample_bytree=0.1,\n",
    "  verbose=-1,\n",
    "  seed=42\n",
    "  )\n",
    "\n",
    "model.fit(train_val[feature_set], train_val[\"target\"])\n",
    "test[\"prediction\"] = model.predict(test[feature_set])\n",
    "\n",
    "\n",
    "\n",
    "# Compute per-era correlation\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "    per_era_corr = validation.groupby(\"era\").apply(\n",
    "        lambda x: numerai_corr(x[[\"prediction\"]].dropna(), x[\"target\"].dropna()),\n",
    "        include_groups=False\n",
    "    )\n",
    "    per_era_corr.fillna(0, inplace=True)\n",
    "\n",
    "# Compute performance metrics\n",
    "corr_mean = per_era_corr.mean()\n",
    "corr_std = per_era_corr.std(ddof=0)\n",
    "corr_sharpe = corr_mean / corr_std if corr_std > 0 else np.nan\n",
    "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "corr_hit_rate = (per_era_corr > 0).mean()\n",
    "\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean:           {corr_mean:>10.6f}\")\n",
    "print(f\"Std:            {corr_std:>10.6f}\")\n",
    "print(f\"Sharpe:         {corr_sharpe:>10.4f}\")\n",
    "print(f\"Max Drawdown:   {corr_max_drawdown:>10.6f}\")\n",
    "print(f\"Hit Rate:       {corr_hit_rate:>10.2%}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Compute feature exposures\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "    feature_exposures = validation.groupby(\"era\").apply(\n",
    "        lambda d: d.drop(columns=[\"target\", \"prediction\"]).corrwith(d[\"prediction\"]),\n",
    "        include_groups=False\n",
    "    )\n",
    "    feature_exposures.fillna(0, inplace=True)\n",
    "\n",
    "# Display feature exposure metrics\n",
    "max_feature_exposure = feature_exposures.max(axis=1)\n",
    "print(\"FEATURE EXPOSURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Max Feature Exposure - Mean: {max_feature_exposure.mean():.4f}\")\n",
    "print(f\"Max Feature Exposure - Std:  {max_feature_exposure.std():.4f}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "\n",
    "# Plot the cumulative per-era correlation\n",
    "per_era_corr.cumsum().plot(\n",
    "title=\"Cumulative Validation CORR\",\n",
    "kind=\"line\",\n",
    "figsize=(8, 4),\n",
    "legend=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
