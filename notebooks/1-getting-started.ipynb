{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf82784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T10:40:36.902544Z",
     "iopub.status.busy": "2025-12-16T10:40:36.901653Z",
     "iopub.status.idle": "2025-12-16T10:40:52.718280Z",
     "shell.execute_reply": "2025-12-16T10:40:52.716982Z"
    },
    "papermill": {
     "duration": 15.823603,
     "end_time": "2025-12-16T10:40:52.720018",
     "exception": false,
     "start_time": "2025-12-16T10:40:36.896415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\r\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\r\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "ydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\r\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install numerapi pyarrow numerai-tools lightgbm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfb0ab",
   "metadata": {
    "papermill": {
     "duration": 0.003777,
     "end_time": "2025-12-16T10:40:52.728373",
     "exception": false,
     "start_time": "2025-12-16T10:40:52.724596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset  \n",
    "\n",
    "At a high level, the Numerai dataset is a tabular dataset that describes the stock market over time. It is compiled from high-quality (and expensive) data that might be difficult for individuals to obtain.\n",
    "\n",
    "The unique thing about Numerai's dataset is that it is `obfuscated`, which means that the underlying stock ids, feature names, and target definitions are anonymized. This makes it so that Numerai can give this data out for free and so that it can be modeled without any financial domain knowledge (or bias!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba73944",
   "metadata": {
    "papermill": {
     "duration": 0.003549,
     "end_time": "2025-12-16T10:40:52.735649",
     "exception": false,
     "start_time": "2025-12-16T10:40:52.732100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Listing the datasets\n",
    "Firstly, take a look at the files Numerai offers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed89d19a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T10:40:52.744578Z",
     "iopub.status.busy": "2025-12-16T10:40:52.744199Z",
     "iopub.status.idle": "2025-12-16T10:40:55.304431Z",
     "shell.execute_reply": "2025-12-16T10:40:55.303282Z"
    },
    "papermill": {
     "duration": 2.566677,
     "end_time": "2025-12-16T10:40:55.305861",
     "exception": false,
     "start_time": "2025-12-16T10:40:52.739184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available versions:\n",
      " ['v5.1', 'v5.0', 'v5.2']\n",
      "Available v5.1 files:\n",
      " ['v5.1/features.json', 'v5.1/live.parquet', 'v5.1/live_benchmark_models.parquet', 'v5.1/live_example_preds.csv', 'v5.1/live_example_preds.parquet', 'v5.1/meta_model.parquet', 'v5.1/train.parquet', 'v5.1/train_benchmark_models.parquet', 'v5.1/validation.parquet', 'v5.1/validation_benchmark_models.parquet', 'v5.1/validation_example_preds.csv', 'v5.1/validation_example_preds.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "# list the datasets and available versions\n",
    "all_datasets = napi.list_datasets()\n",
    "dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
    "print(\"Available versions:\\n\", dataset_versions)\n",
    "\n",
    "# Set data version to one of the latest datasets\n",
    "DATA_VERSION = \"v5.1\"\n",
    "\n",
    "# Print all files available for download for our version\n",
    "current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
    "print(\"Available\", DATA_VERSION, \"files:\\n\", current_version_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a94d8",
   "metadata": {
    "papermill": {
     "duration": 0.003648,
     "end_time": "2025-12-16T10:40:55.313397",
     "exception": false,
     "start_time": "2025-12-16T10:40:55.309749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Downloading datasets\n",
    "\n",
    "The `features.json` file contains metadata about features in the dataset including:\n",
    "- statistics on each feature\n",
    "- helpful sets of features\n",
    "- the targets available for training\n",
    "\n",
    "Let's download it and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71cc9718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T10:40:55.322354Z",
     "iopub.status.busy": "2025-12-16T10:40:55.321661Z",
     "iopub.status.idle": "2025-12-16T10:40:56.203905Z",
     "shell.execute_reply": "2025-12-16T10:40:56.202770Z"
    },
    "papermill": {
     "duration": 0.888768,
     "end_time": "2025-12-16T10:40:56.205545",
     "exception": false,
     "start_time": "2025-12-16T10:40:55.316777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 10:40:55,984 INFO numerapi.utils: starting download\n",
      "v5.1/features.json: 307kB [00:00, 1.58MB/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_sets 18\n",
      "targets 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# download the feature metadata file\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "\n",
    "# read the metadata and display\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "for metadata in feature_metadata:\n",
    "  print(metadata, len(feature_metadata[metadata]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017d16a",
   "metadata": {
    "papermill": {
     "duration": 0.004008,
     "end_time": "2025-12-16T10:40:56.213873",
     "exception": false,
     "start_time": "2025-12-16T10:40:56.209865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Sets & Groups\n",
    "As you can see there are many features and targets to choose from.\n",
    "\n",
    "Instead of training a model on all 2000+ features, let's pick a subset of features to analyze.\n",
    "\n",
    "Here are a few starter sets Numerai offers:\n",
    "\n",
    "- `small` contains a minimal subset of features that have the highest [feature importance](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "\n",
    "- `medium` contains all the \"basic\" features, each unique in some way (e.g. P/E ratios vs analyst ratings)\n",
    "\n",
    "- `all` contains all features in `medium` and their variants (e.g. P/E by country vs P/E by sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09472c60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T10:40:56.225142Z",
     "iopub.status.busy": "2025-12-16T10:40:56.224100Z",
     "iopub.status.idle": "2025-12-16T10:40:56.230140Z",
     "shell.execute_reply": "2025-12-16T10:40:56.229124Z"
    },
    "papermill": {
     "duration": 0.013181,
     "end_time": "2025-12-16T10:40:56.231611",
     "exception": false,
     "start_time": "2025-12-16T10:40:56.218430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small 42\n",
      "medium 740\n",
      "all 2562\n"
     ]
    }
   ],
   "source": [
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "for feature_set in [\"small\", \"medium\", \"all\"]:\n",
    "  print(feature_set, len(feature_sets[feature_set]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4509b4",
   "metadata": {
    "papermill": {
     "duration": 0.004052,
     "end_time": "2025-12-16T10:40:56.240142",
     "exception": false,
     "start_time": "2025-12-16T10:40:56.236090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's load the data. Here we pick the smallest feature set for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460043f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T10:40:56.250989Z",
     "iopub.status.busy": "2025-12-16T10:40:56.250166Z",
     "iopub.status.idle": "2025-12-16T10:44:23.864463Z",
     "shell.execute_reply": "2025-12-16T10:44:23.863087Z"
    },
    "papermill": {
     "duration": 207.621489,
     "end_time": "2025-12-16T10:44:23.865804",
     "exception": true,
     "start_time": "2025-12-16T10:40:56.244315",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 10:40:56,953 INFO numerapi.utils: starting download\n",
      "v5.1/train.parquet: 2.47GB [00:54, 45.0MB/s]                            \n",
      "2025-12-16 10:41:52,687 INFO numerapi.utils: starting download\n",
      "v5.1/validation.parquet: 3.82GB [02:29, 25.6MB/s]                            \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/1920329833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load only the \"medium\" feature set to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Use the \"all\" feature set to use all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m train = pd.read_parquet(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34mf\"{DATA_VERSION}/train.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"era\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             )\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_dataframe\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mindex_descriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index_columns'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_add_any_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         table, index = _reconstruct_index(table, index_descriptors,\n\u001b[0m\u001b[1;32m    795\u001b[0m                                           all_columns, types_mapper)\n\u001b[1;32m    796\u001b[0m         ext_columns_dtypes = _get_extension_dtypes(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_reconstruct_index\u001b[0;34m(table, index_descriptors, all_columns, types_mapper)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdescr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_descriptors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m             result_table, index_level, index_name = _extract_index_level(\n\u001b[0m\u001b[1;32m    993\u001b[0m                 table, result_table, descr, field_name_to_metadata, types_mapper)\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex_level\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_extract_index_level\u001b[0;34m(table, result_table, field_name, field_name_to_metadata, types_mapper)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m     \u001b[0mindex_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_mapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtypes_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0mindex_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     result_table = result_table.remove_column(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.ChunkedArray._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._array_like_to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/pandas-shim.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasAPIShim.series\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.data_manager\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         return sanitize_array(\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 if (\n\u001b[1;32m    608\u001b[0m                     \u001b[0mobject_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;31m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m     return lib.maybe_convert_objects(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define our feature set\n",
    "feature_set = feature_sets[\"small\"]\n",
    "\n",
    "# Download the training data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Download validation data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "validation = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1dd87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f8b3a47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### Training data\n",
    "\n",
    "Each row represents a stock at a specific point in time:\n",
    "- `id` is the stock id\n",
    "- `era` is the date\n",
    "- `target` is a measure of future returns for that stock\n",
    "- `features` describe the attributes of the stock (eg. P/E ratio) for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc908b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b4e93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Eras\n",
    "As mentioned above, each `era` corresponds to a different date. Each era is exactly 1 week apart.\n",
    "\n",
    "It is helpful to think about rows of stocks within the same `era` as a single example. You will notice that throughout this notebook and other examples, we often talk about things \"per era\". For example, the number of rows per era represents the number of stocks in Numerai's investable universe on that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee992b85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the number of rows per era\n",
    "train.groupby(\"era\").size().plot(\n",
    "    title=\"Number of rows per era\",\n",
    "    figsize=(5, 3),\n",
    "    xlabel=\"Era\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ed22b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_eras = train[\"era\"].unique()\n",
    "print(f'There are {len(train_eras)} eras in the training set going from {train_eras[0]} to {train_eras[-1]}')\n",
    "\n",
    "validation_eras = validation[\"era\"].unique()\n",
    "print(f'There are {len(validation_eras)} eras in the validation set going from {validation_eras[0]} to {validation_eras[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ac867",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We're gonna keep the last 100 eras for the test set and only look at them to have our final model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85705d58",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Target\n",
    "The `target` is a measure of stock market returns over the next 20 (business) days. Specifically, it is a measure of \"stock-specific\" returns that are not explained by well-known \"factors\" or broader trends in the market, country, or sector. For example, if Apple went up and the tech sector also went up, we only want to know if Apple went up more or less than the tech sector. This means the target focus on `alpha`, not on `beta`\n",
    "\n",
    "Target values are binned into 5 unequal bins: `0`, `0.25`, `0.5`, `0.75`, `1.0`. Again, this heavy regularization of target values is to avoid overfitting as the underlying values are extremely noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c68bd1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot density histogram of the target\n",
    "train[\"target\"].plot(\n",
    "  kind=\"hist\",\n",
    "  title=\"Target\",\n",
    "  figsize=(5, 3),\n",
    "  xlabel=\"Value\",\n",
    "  density=True,\n",
    "  bins=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c8926",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Features\n",
    "The `features` are quantitative attributes of each stock: fundamentals like P/E ratio, technical signals like RSI, market data like short interest, secondary data like analyst ratings, and much more.\n",
    "\n",
    "The underlying definition of each feature is not important, just know that Numerai has included these features in the dataset because they believe they are predictive of the `target` either by themselves or in combination with other features.\n",
    "\n",
    "Feature values are binned into 5 equal bins: `0`, `1`, `2`, `3`, `4`. This heavy regularization of feature values is to avoid overfitting as the underlying values are extremely noisy. Unlike the target, these are integers instead of floats to reduce the storage needs of the overall dataset.\n",
    "\n",
    "If data for a particular feature is missing for that era (more common in early `eras`), then all values will be set to `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecf400",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "first_era = train[train[\"era\"] == train[\"era\"].unique()[0]]\n",
    "last_era = train[train[\"era\"] == train[\"era\"].unique()[-1]]\n",
    "last_era[feature_set[-1]].plot(\n",
    "   title=\"5 equal bins\",\n",
    "   kind=\"hist\",\n",
    "   density=True,\n",
    "   bins=50,\n",
    "   ax=ax1\n",
    ")\n",
    "first_era[feature_set[-1]].plot(\n",
    "   title=\"missing data\",\n",
    "   kind=\"hist\",\n",
    "   density=True,\n",
    "   bins=50,\n",
    "   ax=ax2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "numerai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 232.922982,
   "end_time": "2025-12-16T10:44:24.573410",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T10:40:31.650428",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
